{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Algorithm Meta-Parameters",
  "technique": "SRI Bayesian Optimization Trigger Search",
  "technique_description": "Search for triggers using Google 1- and 2-gram dataset for Trojan classification. 1) Inference the model with the proposed triggers and record model activations. 2) Record triggers tried and model activations as features, and a Trojan classifier is trained to predict whether the model is Trojaned or not given features. ",
  "technique_changes": "Trigger search schedule generated using the top 200,000 frequent 1-/2-grams from the Google n-gram dataset v2, instead of surrogate-based search schedules based on roberta tokens. We changed the list of 1- and 2-grams by packing them into 20 tokens sequences as trigger candidates, instead of what we did previously, packing 4 1-grams or 2 2-grams. This allows us to run through more 1-/2-grams given the same compute.",
  "commit_id": "d55febe91c2a37f37e20d5a41a835edb2dc07e91",
  "repo_name": "https://github.com/frkl/trojai-fuzzing-nlp",
  "required": ["nclean","bsz","fuzzer_checkpoint"],
  "additionalProperties": false,
  "type": "object",
  "properties": {
    "nclean": {
      "description": "Maximum number of clean examples to use for inference. Default 8",
      "type": "integer",
      "minimum": 1,
      "maximum": 72
    },
    "bsz": {
      "description": "Batch size for inferencing. Only affects speed. Does not affect performance. Change according to GPU VRAM. Default 48",
      "type": "integer",
      "minimum": 1,
      "maximum": 200000
    },
	
    "fuzzer_checkpoint": {
      "description": "Fuzzer checkpoint. schedule_e2s_l8.pt is the pre-extracted trigger search schedule from the Bayesian Optimization model. schedule_rand.pt are random triggers. ",
      "type": "string",
	  "enum": ["schedule_2gram_l20_200k_24267.pt","schedule_1gram_l20_200k_23028.pt","schedule_2gram_l20_500k_62599.pt"]
    }
  }
}

